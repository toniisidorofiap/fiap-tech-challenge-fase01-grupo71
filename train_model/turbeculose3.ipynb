{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gFUeNzuA_VI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7deba4d"
   },
   "source": [
    "# Task\n",
    "Train an AI model for medical exam and clinical document processing with the following objectives: classify exams using Machine Learning on tabular data to diagnose whether a person has a disease, and optionally perform diagnosis with image data using Convolutional Neural Networks (CNN). The solution should involve selecting public medical datasets, exploring and preprocessing the data, building and evaluating classification models using at least two techniques, interpreting the results with techniques like feature importance and SHAP, and critically discussing the practical applicability of the model while emphasizing the physician's final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d138e9c"
   },
   "source": [
    "## Escolha e carregamento dos dados\n",
    "\n",
    "### Subtask:\n",
    "Selecionar um ou mais datasets médicos públicos adequados para o problema de classificação de doenças e carregar os dados em um formato acessível (por exemplo, pandas DataFrame).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "535632d7"
   },
   "source": [
    "# Task\n",
    "Desenvolver um modelo de classificação de imagens utilizando Redes Neurais Convolucionais (CNN) para diagnosticar tuberculose a partir de radiografias de tórax. O modelo deve ser treinado com imagens de exames normais e com tuberculose, disponíveis nos seguintes links do Google Drive: \"https://drive.google.com/drive/folders/1jCPGtOqr--sK3lXdefqd0AcS3fKaIL2S?usp=drive_link\" (exames normais) e \"https://drive.google.com/drive/folders/13DxNeyOJIJvCIAHfkOtrfzZAS-_6OB9l?usp=drive_link\" (exames com tuberculose). O projeto deve incluir o carregamento e organização das imagens, pré-processamento, construção e treinamento da CNN, avaliação do modelo com métricas adequadas e uma discussão crítica dos resultados e sua aplicabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6ee9e20"
   },
   "source": [
    "## Montar o google drive\n",
    "\n",
    "### Subtask:\n",
    "Acessar os arquivos de imagem diretamente do Google Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be79ed31"
   },
   "source": [
    "**Reasoning**:\n",
    "Import the necessary library and mount Google Drive to access the image files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "executionInfo": {
     "elapsed": 7851,
     "status": "error",
     "timestamp": 1761496923207,
     "user": {
      "displayName": "Toni Isidoro",
      "userId": "07382729379577168648"
     },
     "user_tz": 180
    },
    "id": "42f19d3f",
    "outputId": "57c6eec7-47b6-4cc6-da2e-8875a19572a6"
   },
   "outputs": [
    {
     "ename": "MessageError",
     "evalue": "Error: credential propagation was unsuccessful",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(__file__).parent / \"dataset\"\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"tawsifurrahman/tuberculosis-tb-chest-xray-dataset\", path=dataset_path)\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb129b0b"
   },
   "source": [
    "## Carregar e organizar as imagens\n",
    "\n",
    "### Subtask:\n",
    "Carregar as imagens dos links fornecidos (radiografias normais e com tuberculose) e organizá-las em estruturas de diretório adequadas para o treinamento de modelos de Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d53a564"
   },
   "source": [
    "**Reasoning**:\n",
    "Create the necessary directories and copy the images from Google Drive to these directories, then count the number of images in each directory to verify the copy operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "572e6bae"
   },
   "source": [
    "**Reasoning**:\n",
    "Instantiate an ImageDataGenerator for training data with rescaling and data augmentation, and another for validation data with only rescaling, both with a validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0add5600"
   },
   "source": [
    "**Reasoning**:\n",
    "Use the ImageDataGenerator objects to create data generators for training and validation by flowing from the image directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc64c3ba"
   },
   "source": [
    "**Reasoning**:\n",
    "Create the training and validation data generators using flow_from_directory and print the number of images found to verify the data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1761392920285,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "8a61fa50",
    "outputId": "872ca09c-1e63-4d8d-b63c-5e9035b7194f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of /content/data/normal:\n",
      "['Normal-2029.png', 'Normal-407.png', 'Normal-1936.png', 'Normal-2413.png', 'Normal-3410.png', 'Normal-859.png', 'Normal-490.png', 'Normal-934.png', 'Normal-973.png', 'Normal-1421.png']\n",
      "\n",
      "Number of files in /content/data/normal: 3500\n",
      "\n",
      "Contents of /content/data/tuberculosis:\n",
      "['Tuberculosis-132.png', 'Tuberculosis-128.png', 'Tuberculosis-133.png', 'Tuberculosis-168.png', 'Tuberculosis-149.png', 'Tuberculosis-113.png', 'Tuberculosis-111.png', 'Tuberculosis-137.png', 'Tuberculosis-138.png']\n",
      "\n",
      "Number of files in /content/data/tuberculosis: 9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "normal_dir = '/content/data/normal'\n",
    "tuberculosis_dir = '/content/data/tuberculosis'\n",
    "\n",
    "print(f\"Contents of {normal_dir}:\")\n",
    "print(os.listdir(normal_dir)[:10]) # Print first 10 files to avoid long output\n",
    "print(f\"\\nNumber of files in {normal_dir}: {len(os.listdir(normal_dir))}\")\n",
    "\n",
    "print(f\"\\nContents of {tuberculosis_dir}:\")\n",
    "print(os.listdir(tuberculosis_dir)[:10]) # Print first 10 files to avoid long output\n",
    "print(f\"\\nNumber of files in {tuberculosis_dir}: {len(os.listdir(tuberculosis_dir))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70e23fd8"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous attempts to load and organize the images failed because the source paths were incorrect. The user needs to update the `source_normal` and `source_tuberculosis` variables with the correct paths to their image folders in Google Drive. The provided code block includes instructions for the user to update these paths. Executing this block again with the correct paths should allow the images to be copied, thus completing the subtask. I am now updating the paths based on the user's previous input and adding code to clear the destination directories before copying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ea2fa50"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous command failed because the specified paths for the image folders in Google Drive were incorrect. I need to provide the correct paths to the folders based on the Google Drive links provided in the task description. Since I don't have direct access to the Google Drive file structure from the links, I will assume the user needs to update the `source_normal` and `source_tuberculosis` variables with the actual paths to the folders after mounting Google Drive. I will regenerate the code block with comments indicating where the user should update the paths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1761392920299,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "DlETjA0qCby7",
    "outputId": "720560c1-0653-4ded-b7d7-3d5220d65edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 2] No such file or directory: '/content/drive/MyDrive/path/to/your/normal/images'. Please ensure the source paths in Google Drive are correct.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/data/normal', exist_ok=True)\n",
    "os.makedirs('/content/data/tuberculosis', exist_ok=True)\n",
    "\n",
    "# Define source directories in Google Drive.\n",
    "# **IMPORTANT**: Replace the paths below with the actual paths to your image folders\n",
    "# after mounting your Google Drive.\n",
    "# You can find the correct paths by navigating your mounted Google Drive in the Colab file explorer.\n",
    "source_normal = '/content/drive/MyDrive/path/to/your/normal/images' # Update this path\n",
    "source_tuberculosis = '/content/drive/MyDrive/path/to/your/tuberculosis/images' # Update this path\n",
    "\n",
    "# Copy images from Google Drive to the new directories\n",
    "try:\n",
    "    for filename in os.listdir(source_normal):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_normal, filename), '/content/data/normal')\n",
    "\n",
    "    for filename in os.listdir(source_tuberculosis):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_tuberculosis, filename), '/content/data/tuberculosis')\n",
    "\n",
    "    # Verify the copy operation by counting files\n",
    "    num_normal = len(os.listdir('/content/data/normal'))\n",
    "    num_tuberculosis = len(os.listdir('/content/data/tuberculosis'))\n",
    "\n",
    "    print(f\"Number of normal images copied: {num_normal}\")\n",
    "    print(f\"Number of tuberculosis images copied: {num_tuberculosis}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure the source paths in Google Drive are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f5b5343"
   },
   "source": [
    "## Pré-processamento das imagens\n",
    "\n",
    "### Subtask:\n",
    "Realizar o pré-processamento das imagens, incluindo redimensionamento, normalização e possivelmente aumento de dados (data augmentation) para preparar o dataset para a CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6ca49ae"
   },
   "source": [
    "**Reasoning**:\n",
    "Import necessary libraries and define parameters for image preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d47dc6e6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8244ee6d"
   },
   "source": [
    "**Reasoning**:\n",
    "Create ImageDataGenerators for training and validation sets, including data augmentation for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6221a15"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    validation_split=0.2 # Split data for validation\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2 # Use the same split as training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0c56c03"
   },
   "source": [
    "**Reasoning**:\n",
    "Use the ImageDataGenerators to load images from the directories, applying preprocessing and creating data generators for training and validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1761392920347,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "b807dfed",
    "outputId": "5a26bdf8-842d-496a-d027-38e466578db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2808 images belonging to 2 classes.\n",
      "Found 701 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/content/data'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False # No need to shuffle validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78b18573"
   },
   "source": [
    "## Carregar e organizar as imagens\n",
    "\n",
    "### Subtask:\n",
    "Carregar as imagens dos links fornecidos (radiografias normais e com tuberculose) e organizá-las em estruturas de diretório adequadas para o treinamento de modelos de Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f28a3ce"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous attempt to load and organize the images failed because the source paths were incorrect. I need to update the source paths to the correct location of the images in the mounted Google Drive, as instructed in the prompt, and then execute the code to copy the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1761392920370,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "d8171256",
    "outputId": "f9f69959-fff9-4431-c03e-e9ec044eadf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 2] No such file or directory: '/content/drive/MyDrive/normal'. Please ensure the source paths in Google Drive are correct.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/data/normal', exist_ok=True)\n",
    "os.makedirs('/content/data/tuberculosis', exist_ok=True)\n",
    "\n",
    "# Define source directories in Google Drive.\n",
    "# **IMPORTANT**: Replace the paths below with the actual paths to your image folders\n",
    "# after mounting your Google Drive.\n",
    "# You can find the correct paths by navigating your mounted Google Drive in the Colab file explorer.\n",
    "source_normal = '/content/drive/MyDrive/normal' # Update this path\n",
    "source_tuberculosis = '/content/drive/MyDrive/tuberculosis' # Update this path\n",
    "\n",
    "# Copy images from Google Drive to the new directories\n",
    "try:\n",
    "    for filename in os.listdir(source_normal):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_normal, filename), '/content/data/normal')\n",
    "\n",
    "    for filename in os.listdir(source_tuberculosis):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_tuberculosis, filename), '/content/data/tuberculosis')\n",
    "\n",
    "    # Verify the copy operation by counting files\n",
    "    num_normal = len(os.listdir('/content/data/normal'))\n",
    "    num_tuberculosis = len(os.listdir('/content/data/tuberculosis'))\n",
    "\n",
    "    print(f\"Number of normal images copied: {num_normal}\")\n",
    "    print(f\"Number of tuberculosis images copied: {num_tuberculosis}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure the source paths in Google Drive are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b440603"
   },
   "source": [
    "## Pré-processamento das imagens\n",
    "\n",
    "### Subtask:\n",
    "Realizar o pré-processamento das imagens, incluindo redimensionamento, normalização e possivelmente aumento de dados (data augmentation) para preparar o dataset para a CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1943884f"
   },
   "source": [
    "## Carregar e organizar as imagens\n",
    "\n",
    "### Subtask:\n",
    "Carregar as imagens dos links fornecidos (radiografias normais e com tuberculose) e organizá-las em estruturas de diretório adequadas para o treinamento de modelos de Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bea1f55"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous attempt to load and organize the images failed because the source paths were incorrect. The user needs to update the `source_normal` and `source_tuberculosis` variables with the correct paths to their image folders in Google Drive. The provided code block includes instructions for the user to update these paths. Executing this block again with the correct paths should allow the images to be copied, thus completing the subtask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94493,
     "status": "ok",
     "timestamp": 1761393014864,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "757793ef",
    "outputId": "70874fbe-dae4-4f3a-d535-a9212da449ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to copy images from: /content/drive/MyDrive/turbeculose/TB_Chest_Radiography_Database/Normal and /content/drive/MyDrive/turbeculose/TB_Chest_Radiography_Database/Tuberculosis\n",
      "Number of normal images copied: 3500\n",
      "Number of tuberculosis images copied: 700\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/data/normal', exist_ok=True)\n",
    "os.makedirs('/content/data/tuberculosis', exist_ok=True)\n",
    "\n",
    "# Define source directories in Google Drive.\n",
    "# **IMPORTANT**: Replace the paths below with the actual paths to your image folders\n",
    "# after mounting your Google Drive.\n",
    "# You can find the correct paths by navigating your mounted Google Drive in the Colab file explorer.\n",
    "source_normal = '/content/drive/MyDrive/turbeculose/TB_Chest_Radiography_Database/Normal' # Updated path based on the new link\n",
    "source_tuberculosis = '/content/drive/MyDrive/turbeculose/TB_Chest_Radiography_Database/Tuberculosis' # Updated path based on the new link\n",
    "\n",
    "\n",
    "# Copy images from Google Drive to the new directories\n",
    "try:\n",
    "    print(f\"Attempting to copy images from: {source_normal} and {source_tuberculosis}\")\n",
    "    # Clear the destination directories before copying to avoid duplicating files on subsequent runs\n",
    "    shutil.rmtree('/content/data/normal', ignore_errors=True)\n",
    "    shutil.rmtree('/content/data/tuberculosis', ignore_errors=True)\n",
    "    os.makedirs('/content/data/normal', exist_ok=True)\n",
    "    os.makedirs('/content/data/tuberculosis', exist_ok=True)\n",
    "\n",
    "\n",
    "    for filename in os.listdir(source_normal):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_normal, filename), '/content/data/normal')\n",
    "\n",
    "    for filename in os.listdir(source_tuberculosis):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_tuberculosis, filename), '/content/data/tuberculosis')\n",
    "\n",
    "    # Verify the copy operation by counting files\n",
    "    num_normal = len(os.listdir('/content/data/normal'))\n",
    "    num_tuberculosis = len(os.listdir('/content/data/tuberculosis'))\n",
    "\n",
    "    print(f\"Number of normal images copied: {num_normal}\")\n",
    "    print(f\"Number of tuberculosis images copied: {num_tuberculosis}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure the source paths in Google Drive are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a17442cb"
   },
   "source": [
    "## Pré-processamento das imagens\n",
    "\n",
    "### Subtask:\n",
    "Realizar o pré-processamento das imagens, incluindo redimensionamento, normalização e possivelmente aumento de dados (data augmentation) para preparar o dataset para a CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e78aa718"
   },
   "source": [
    "**Reasoning**:\n",
    "Instantiate an ImageDataGenerator for training data with rescaling and data augmentation, and another for validation data with only rescaling, both with a validation split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4422beb1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfb9963c"
   },
   "source": [
    "**Reasoning**:\n",
    "Use the ImageDataGenerator objects to create data generators for training and validation by flowing from the image directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1761393014937,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "894a8b7c",
    "outputId": "a1eb6a94-a021-445f-f2b1-dc318f4a203e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3360 images belonging to 2 classes.\n",
      "Found 840 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_dir = '/content/data'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4412568f"
   },
   "source": [
    "## Construir o modelo cnn\n",
    "\n",
    "### Subtask:\n",
    "Definir a arquitetura da Rede Neural Convolucional (CNN) para a tarefa de classificação de imagens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfa96c56"
   },
   "source": [
    "**Reasoning**:\n",
    "Import the necessary modules from TensorFlow and Keras to define the CNN architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b738d66c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3416abbf"
   },
   "source": [
    "**Reasoning**:\n",
    "Instantiate a Sequential model and add the convolutional and pooling layers as described in the instructions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1761393015069,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "ccc4cddd",
    "outputId": "04f59def-964f-4d79-d4a8-34f6a9db58d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d0a65f4"
   },
   "source": [
    "**Reasoning**:\n",
    "Flatten the output of the convolutional layers and add dense layers for classification, then print the model summary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1761393015360,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "1QtuO4mLC8fl",
    "outputId": "510c4bc7-51f0-4a23-c741-48eac630f4c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86528</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,075,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86528\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m11,075,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,169,089</span> (42.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,169,089\u001b[0m (42.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,169,089</span> (42.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,169,089\u001b[0m (42.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a16cfe81"
   },
   "source": [
    "## Separar os dados\n",
    "\n",
    "### Subtask:\n",
    "Dividir o dataset de imagens em conjuntos de treino, validação e teste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "950a52a4"
   },
   "source": [
    "**Reasoning**:\n",
    "Create the test data generator using ImageDataGenerator, ensuring it uses the 'validation' subset and shuffle is set to False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761393015370,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "cd36d5da",
    "outputId": "99626233-93ea-4e21-d799-83b449fecbc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255) # Only rescaling needed for test data\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation', # Use the validation subset for testing as per the instructions\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a165118"
   },
   "source": [
    "## Carregar e organizar as imagens\n",
    "\n",
    "### Subtask:\n",
    "Carregar as imagens dos links fornecidos (radiografias normais e com tuberculose) e organizá-las em estruturas de diretório adequadas para o treinamento de modelos de Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "254abaf0"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous attempts to load and organize the images failed because the source paths in Google Drive were incorrect. I need to provide the code again with instructions for the user to update the paths and execute it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761393015384,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "524252c9",
    "outputId": "cb4cc5f3-931e-424b-fc94-66ec5ec98dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to copy images from: /content/drive/MyDrive/Normal and /content/drive/MyDrive/Tuberculosis\n",
      "Error: [Errno 2] No such file or directory: '/content/drive/MyDrive/Normal'. Please ensure the source paths in Google Drive are correct.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/data/normal', exist_ok=True)\n",
    "os.makedirs('/content/data/tuberculosis', exist_ok=True)\n",
    "\n",
    "# Define source directories in Google Drive.\n",
    "# **IMPORTANT**: Replace the paths below with the actual paths to your image folders\n",
    "# after mounting your Google Drive.\n",
    "# You can find the correct paths by navigating your mounted Google Drive in the Colab file explorer.\n",
    "source_normal = '/content/drive/MyDrive/Normal' # Update this path\n",
    "source_tuberculosis = '/content/drive/MyDrive/Tuberculosis' # Update this path\n",
    "\n",
    "\n",
    "# Copy images from Google Drive to the new directories\n",
    "try:\n",
    "    print(f\"Attempting to copy images from: {source_normal} and {source_tuberculosis}\")\n",
    "    for filename in os.listdir(source_normal):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_normal, filename), '/content/data/normal')\n",
    "\n",
    "    for filename in os.listdir(source_tuberculosis):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_tuberculosis, filename), '/content/data/tuberculosis')\n",
    "\n",
    "    # Verify the copy operation by counting files\n",
    "    num_normal = len(os.listdir('/content/data/normal'))\n",
    "    num_tuberculosis = len(os.listdir('/content/data/tuberculosis'))\n",
    "\n",
    "    print(f\"Number of normal images copied: {num_normal}\")\n",
    "    print(f\"Number of tuberculosis images copied: {num_tuberculosis}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure the source paths in Google Drive are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5311304d"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous attempt to copy the images failed again because the source paths were incorrect. I need to reiterate the instruction for the user to correct the paths in Google Drive and then try copying the files again. Since I cannot automatically find the correct paths, I must rely on the user to provide them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1761393015408,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "77907034",
    "outputId": "76f8f165-fb27-4686-9b34-0f59a18f29d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to copy images from: /content/drive/MyDrive/Normal and /content/drive/MyDrive/Tuberculosis\n",
      "Error: [Errno 2] No such file or directory: '/content/drive/MyDrive/Normal'. Please ensure the source paths in Google Drive are correct.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/data/normal', exist_ok=True)\n",
    "os.makedirs('/content/data/tuberculosis', exist_ok=True)\n",
    "\n",
    "# Define source directories in Google Drive.\n",
    "# **IMPORTANT**: Replace the paths below with the actual paths to your image folders\n",
    "# after mounting your Google Drive.\n",
    "# You can find the correct paths by navigating your mounted Google Drive in the Colab file explorer.\n",
    "source_normal = '/content/drive/MyDrive/Normal' # Update this path\n",
    "source_tuberculosis = '/content/drive/MyDrive/Tuberculosis' # Update this path\n",
    "\n",
    "\n",
    "# Copy images from Google Drive to the new directories\n",
    "try:\n",
    "    print(f\"Attempting to copy images from: {source_normal} and {source_tuberculosis}\")\n",
    "    for filename in os.listdir(source_normal):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_normal, filename), '/content/data/normal')\n",
    "\n",
    "    for filename in os.listdir(source_tuberculosis):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_tuberculosis, filename), '/content/data/tuberculosis')\n",
    "\n",
    "    # Verify the copy operation by counting files\n",
    "    num_normal = len(os.listdir('/content/data/normal'))\n",
    "    num_tuberculosis = len(os.listdir('/content/data/tuberculosis'))\n",
    "\n",
    "    print(f\"Number of normal images copied: {num_normal}\")\n",
    "    print(f\"Number of tuberculosis images copied: {num_tuberculosis}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure the source paths in Google Drive are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84d8fbea"
   },
   "source": [
    "## Pré-processamento das imagens\n",
    "\n",
    "### Subtask:\n",
    "Realizar o pré-processamento das imagens, incluindo redimensionamento, normalização e possivelmente aumento de dados (data augmentation) para preparar o dataset para a CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61d94de3"
   },
   "source": [
    "**Reasoning**:\n",
    "Import the necessary libraries for image preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f976dd78"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02fb993a"
   },
   "source": [
    "**Reasoning**:\n",
    "Define the ImageDataGenerator for training and validation sets, including data augmentation for training and normalization for both.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc38747d"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    validation_split=0.2 # Split data for validation\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2 # Use the same split as training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80200d1c"
   },
   "source": [
    "**Reasoning**:\n",
    "Create the training and validation data generators using flow_from_directory and print the number of images found to verify the data loading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1761393015491,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "43e692a3",
    "outputId": "0da5e27f-2960-45f4-bcac-e9db5e9ec16f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3360 images belonging to 2 classes.\n",
      "Found 840 images belonging to 2 classes.\n",
      "Found 3360 training images belonging to 2 classes.\n",
      "Found 840 validation images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/content/data'\n",
    "\n",
    "# Assuming train_datagen and validation_datagen are already defined and configured\n",
    "# from previous steps\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False # No need to shuffle validation data\n",
    ")\n",
    "\n",
    "print(f\"Found {train_generator.samples} training images belonging to {train_generator.num_classes} classes.\")\n",
    "print(f\"Found {validation_generator.samples} validation images belonging to {validation_generator.num_classes} classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5862c3a9"
   },
   "source": [
    "## Construir o modelo cnn\n",
    "\n",
    "### Subtask:\n",
    "Definir a arquitetura da Rede Neural Convolucional (CNN) para a tarefa de classificação de imagens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66fbae44"
   },
   "source": [
    "**Reasoning**:\n",
    "Define the CNN architecture as per the instructions, including the input shape for the first convolutional layer and print the model summary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1761393015803,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "aa2dd76d",
    "outputId": "cd419ae9-67ca-4eb1-c972-47083fa1376f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86528</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,075,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86528\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m11,075,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,169,089</span> (42.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,169,089\u001b[0m (42.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,169,089</span> (42.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,169,089\u001b[0m (42.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f0ded46"
   },
   "source": [
    "## Separar os dados\n",
    "\n",
    "### Subtask:\n",
    "Dividir o dataset de imagens em conjuntos de treino, validação e teste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14ca91e9"
   },
   "source": [
    "**Reasoning**:\n",
    "Import the ImageDataGenerator class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90d2b3ed"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0aefef5"
   },
   "source": [
    "**Reasoning**:\n",
    "Instantiate an ImageDataGenerator for the test set and create the test generator using flow_from_directory with the specified parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1761393015816,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "7659a3f1",
    "outputId": "d3996d04-92e4-4554-a342-1a044c87eb4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255) # Only rescaling needed for test data\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '/content/data',\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation', # Using validation subset as per instructions for the test generator\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b130fbe9"
   },
   "source": [
    "**Reasoning**:\n",
    "Print the number of images and classes found for the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1761393015820,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "05ce81cf",
    "outputId": "780d817d-9b6e-4858-9cff-bc1d9de45d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 test images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {test_generator.samples} test images belonging to {test_generator.num_classes} classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68126e0b"
   },
   "source": [
    "## Carregar e organizar as imagens\n",
    "\n",
    "### Subtask:\n",
    "Carregar as imagens dos links fornecidos (radiografias normais e com tuberculose) e organizá-las em estruturas de diretório adequadas para o treinamento de modelos de Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1adc6ecc"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous attempts to load and organize the images failed because the source paths in Google Drive were incorrect. I need to provide the code again, emphasizing that the user must update the placeholder paths with the correct ones from their mounted Google Drive. This is the only way to proceed with the task of loading the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1761393015876,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "76f436cc",
    "outputId": "887a3a29-f99e-4573-f9f2-198b49f25e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to copy images from: /content/drive/MyDrive/Normal and /content/drive/MyDrive/Tuberculosis\n",
      "Error: [Errno 2] No such file or directory: '/content/drive/MyDrive/Normal'. Please ensure the source paths in Google Drive are correct.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/data/normal', exist_ok=True)\n",
    "os.makedirs('/content/data/tuberculosis', exist_ok=True)\n",
    "\n",
    "# Define source directories in Google Drive.\n",
    "# **IMPORTANT**: Replace the paths below with the actual paths to your image folders\n",
    "# after mounting your Google Drive.\n",
    "# You can find the correct paths by navigating your mounted Google Drive in the Colab file explorer.\n",
    "source_normal = '/content/drive/MyDrive/Normal' # Update this path\n",
    "source_tuberculosis = '/content/drive/MyDrive/Tuberculosis' # Update this path\n",
    "\n",
    "\n",
    "# Copy images from Google Drive to the new directories\n",
    "try:\n",
    "    print(f\"Attempting to copy images from: {source_normal} and {source_tuberculosis}\")\n",
    "    for filename in os.listdir(source_normal):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_normal, filename), '/content/data/normal')\n",
    "\n",
    "    for filename in os.listdir(source_tuberculosis):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_tuberculosis, filename), '/content/data/tuberculosis')\n",
    "\n",
    "    # Verify the copy operation by counting files\n",
    "    num_normal = len(os.listdir('/content/data/normal'))\n",
    "    num_tuberculosis = len(os.listdir('/content/data/tuberculosis'))\n",
    "\n",
    "    print(f\"Number of normal images copied: {num_normal}\")\n",
    "    print(f\"Number of tuberculosis images copied: {num_tuberculosis}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure the source paths in Google Drive are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bf1ca28"
   },
   "source": [
    "## Carregar e organizar as imagens\n",
    "\n",
    "### Subtask:\n",
    "Carregar as imagens dos links fornecidos (radiografias normais e com tuberculose) e organizá-las em estruturas de diretório adequadas para o treinamento de modelos de Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24b68117"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous attempts to load and organize the images failed because the source paths in Google Drive were incorrect. I need to provide the code again, emphasizing that the user *must* manually update the `source_normal` and `source_tuberculosis` variables with the correct paths to their image folders in Google Drive. I will keep the rest of the logic for creating directories and copying files as before, including the error handling and verification steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27599,
     "status": "ok",
     "timestamp": 1761393043475,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "7278cb00",
    "outputId": "f08862d4-3dae-4c7f-9c57-74ac05fa8742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to copy images from: /content/drive/MyDrive/turbeculose/TB_Chest_Radiography_Database/Normal and /content/drive/MyDrive/turbeculose/TB_Chest_Radiography_Database/Tuberculosis\n",
      "Number of normal images copied: 3500\n",
      "Number of tuberculosis images copied: 700\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/data/normal', exist_ok=True)\n",
    "os.makedirs('/content/data/tuberculosis', exist_ok=True)\n",
    "\n",
    "# Define source directories in Google Drive.\n",
    "# **IMPORTANT**: Replace the paths below with the actual paths to your image folders\n",
    "# after mounting your Google Drive.\n",
    "# You can find the correct paths by navigating your mounted Google Drive in the Colab file explorer.\n",
    "source_normal = '/content/drive/MyDrive/turbeculose/TB_Chest_Radiography_Database/Normal' # Update this path with the correct path to your normal images\n",
    "source_tuberculosis = '/content/drive/MyDrive/turbeculose/TB_Chest_Radiography_Database/Tuberculosis' # Updated path based on the new link\n",
    "\n",
    "\n",
    "# Copy images from Google Drive to the new directories\n",
    "try:\n",
    "    print(f\"Attempting to copy images from: {source_normal} and {source_tuberculosis}\")\n",
    "    # Clear the destination directories before copying to avoid duplicating files on subsequent runs\n",
    "    shutil.rmtree('/content/data/normal', ignore_errors=True)\n",
    "    shutil.rmtree('/content/data/tuberculosis', ignore_errors=True)\n",
    "    os.makedirs('/content/data/normal', exist_ok=True)\n",
    "    os.makedirs('/content/data/tuberculosis', exist_ok=True)\n",
    "\n",
    "\n",
    "    for filename in os.listdir(source_normal):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_normal, filename), '/content/data/normal')\n",
    "\n",
    "    for filename in os.listdir(source_tuberculosis):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            shutil.copy(os.path.join(source_tuberculosis, filename), '/content/data/tuberculosis')\n",
    "\n",
    "    # Verify the copy operation by counting files\n",
    "    num_normal = len(os.listdir('/content/data/normal'))\n",
    "    num_tuberculosis = len(os.listdir('/content/data/tuberculosis'))\n",
    "\n",
    "    print(f\"Number of normal images copied: {num_normal}\")\n",
    "    print(f\"Number of tuberculosis images copied: {num_tuberculosis}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure the source paths in Google Drive are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99303818"
   },
   "source": [
    "## Pré-processamento das imagens\n",
    "\n",
    "### Subtask:\n",
    "Realizar o pré-processamento das imagens, incluindo redimensionamento, normalização e possivelmente aumento de dados (data augmentation) para preparar o dataset para a CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f4e616e"
   },
   "source": [
    "**Reasoning**:\n",
    "Instantiate the training and validation ImageDataGenerators and create the image data generators using flow_from_directory, then print the number of images found.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1761393043515,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "9858e3d5",
    "outputId": "51d55b25-490d-4c94-a832-7422eed7979b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3360 images belonging to 2 classes.\n",
      "Found 840 images belonging to 2 classes.\n",
      "Found 3360 training images belonging to 2 classes.\n",
      "Found 840 validation images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "data_dir = '/content/data'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Found {train_generator.samples} training images belonging to {train_generator.num_classes} classes.\")\n",
    "print(f\"Found {validation_generator.samples} validation images belonging to {validation_generator.num_classes} classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5054338a"
   },
   "source": [
    "## Treinar o modelo cnn\n",
    "\n",
    "### Subtask:\n",
    "Compilar e treinar o modelo CNN utilizando os dados de treino e validação.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccb6a111"
   },
   "source": [
    "**Reasoning**:\n",
    "Compile the previously defined CNN model with the specified optimizer, loss function, and metrics, then train it using the training and validation data generators for a fixed number of epochs, storing the training history.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7249445,
     "status": "ok",
     "timestamp": 1761400292970,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "88f1bca1",
    "outputId": "25d61fdd-4bd8-487a-abaf-9320487e0976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3360 images belonging to 2 classes.\n",
      "Found 840 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 4s/step - accuracy: 0.8145 - loss: 0.5559 - val_accuracy: 0.8582 - val_loss: 0.3837\n",
      "Epoch 2/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 4s/step - accuracy: 0.9151 - loss: 0.2223 - val_accuracy: 0.8702 - val_loss: 0.2488\n",
      "Epoch 3/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 4s/step - accuracy: 0.9300 - loss: 0.1760 - val_accuracy: 0.8702 - val_loss: 0.2992\n",
      "Epoch 4/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 4s/step - accuracy: 0.9411 - loss: 0.1445 - val_accuracy: 0.8834 - val_loss: 0.2168\n",
      "Epoch 5/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 4s/step - accuracy: 0.9283 - loss: 0.1854 - val_accuracy: 0.8438 - val_loss: 0.2730\n",
      "Epoch 6/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 5s/step - accuracy: 0.8525 - loss: 0.3649 - val_accuracy: 0.8762 - val_loss: 0.2792\n",
      "Epoch 7/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 5s/step - accuracy: 0.8719 - loss: 0.3267 - val_accuracy: 0.8690 - val_loss: 0.3124\n",
      "Epoch 8/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 4s/step - accuracy: 0.9186 - loss: 0.2110 - val_accuracy: 0.8726 - val_loss: 0.2694\n",
      "Epoch 9/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 4s/step - accuracy: 0.9219 - loss: 0.1890 - val_accuracy: 0.8690 - val_loss: 0.3587\n",
      "Epoch 10/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 5s/step - accuracy: 0.9344 - loss: 0.1672 - val_accuracy: 0.8738 - val_loss: 0.2293\n",
      "Epoch 11/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 5s/step - accuracy: 0.9471 - loss: 0.1463 - val_accuracy: 0.8738 - val_loss: 0.2487\n",
      "Epoch 12/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 4s/step - accuracy: 0.9429 - loss: 0.1530 - val_accuracy: 0.8726 - val_loss: 0.3071\n",
      "Epoch 13/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 4s/step - accuracy: 0.9208 - loss: 0.1948 - val_accuracy: 0.9147 - val_loss: 0.1844\n",
      "Epoch 14/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 5s/step - accuracy: 0.9275 - loss: 0.1867 - val_accuracy: 0.8750 - val_loss: 0.2643\n",
      "Epoch 15/15\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 4s/step - accuracy: 0.9479 - loss: 0.1363 - val_accuracy: 0.9014 - val_loss: 0.1967\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "data_dir = '/content/data'\n",
    "\n",
    "# Create ImageDataGenerators for training and validation sets\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    validation_split=0.2 # Split data for validation\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2 # Use the same split as training\n",
    ")\n",
    "\n",
    "# Create data generators using flow_from_directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False # No need to shuffle validation data\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=15, # Training for 15 epochs\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41919,
     "status": "ok",
     "timestamp": 1761400843124,
     "user": {
      "displayName": "Marconi Alves",
      "userId": "00455870952690527970"
     },
     "user_tz": 180
    },
    "id": "292e4809",
    "outputId": "87604389-20bf-4fb0-b28f-89834fbef0f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.9778 - loss: 0.0599\n",
      "Validation Loss: 0.1982\n",
      "Validation Accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "\n",
    "print(f\"Validation Loss: {loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
